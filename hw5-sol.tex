\documentclass[11pt]{article}
\usepackage{color, epsfig, latexsym}
\usepackage[colorlinks=true, pdfstartview=FitV, linkcolor=blue, 
            citecolor=blue, urlcolor=blue]{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{hhline}
\usepackage[retainorgcmds]{IEEEtrantools}

\renewcommand{\baselinestretch}{1}
\addtolength{\topmargin}{-2cm}
\addtolength{\oddsidemargin}{-1.5cm}
\addtolength{\textheight}{3cm}
\addtolength{\textwidth}{4cm}
\setlength{\parskip}{0.3cm}
\setlength{\parindent}{0cm}

%\usepackage{../../templates/jointhomework}


\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}

\newcommand{\bnum}{\begin{enumerate}}
\newcommand{\enum}{\end{enumerate}}

\newcommand{\beq}{\begin{eqnarray*}}
\newcommand{\eeq}{\end{eqnarray*}}

\newcommand{\beqn}{\begin{eqnarray}}
\newcommand{\eeqn}{\end{eqnarray}}

\newcommand{\key}[1]{\textbf{#1}}

\newcommand{\p}{\mbox{P}}



\def\btheta{\mbox{\boldmath $\theta$}}
\def\bmu{\mbox{\boldmath $\mu$}}
\def\bgam{\mbox{\boldmath $\gamma$}}
\def\bsig{\mbox{\boldmath $\sigma$}}
\def\bomega{\mbox{\boldmath $\omega$}}
\def\bdeta{\mbox{\boldmath $\eta$}}
\def\bpi{\mbox{\boldmath $\pi$}}
\def\bbeta{\mbox{\boldmath $\beta$}}

\newcommand{\bY}{\mbox{\boldmath $Y$}}
\newcommand{\by}{\mbox{\boldmath $y$}}

\newcommand{\bI}{\mbox{\boldmath $I$}}

\newcommand{\bX}{\mbox{\boldmath $X$}}

\newcommand{\bx}{\mbox{\boldmath $x$}}
\newcommand{\bn}{\mbox{\boldmath $n$}}
\newcommand{\bN}{\mbox{\boldmath $N$}}
\newcommand{\bc}{\mbox{\boldmath $c$}}
\newcommand{\bd}{\mbox{\boldmath $d$}}
\newcommand{\tbc}{\widetilde{\mbox{\boldmath $c$}}}
\newcommand{\bchat}{\widehat{\mbox{\boldmath $c$}}}
\newcommand{\bZ}{\mbox{\boldmath $Z$}}
\newcommand{\bL}{\mbox{\boldmath $L$}}
\newcommand{\bM}{\mbox{\boldmath $M$}}
\newcommand{\bz}{\mbox{\boldmath $z$}}
\newcommand{\bell}{\mbox{\boldmath $\ell$}}
%\renewcommand {\bpi} {\mbox{\boldmath $\pi$}}
\newcommand{\No} {\text{N}}
\newcommand {\bg} {\mbox{\boldmath $g$}}
\newcommand{\Bern} {\text{Bern}}

\newcommand{\Exp} {\text{E}}
\newcommand{\Cov} {\text{Cov}}

\newcommand{\bV}{\mbox{\boldmath $V$}}
\newcommand{\bv}{\mbox{\boldmath $v$}}
\newcommand{\brr}{\mbox{\boldmath $r$}}
\newcommand {\bh} {\mbox{\boldmath $h$}}

\newcommand{\bw}{\mbox{\boldmath $w$}}
\newcommand{\bp}{\mbox{\boldmath $p$}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bnu}{\bm{\nu}}
\newcommand{\Sbar}{\overline{S}}
\newcommand{\Var} {\text{Var}}

\newcommand{\Ga}{\mbox{Gamma}}
\newcommand{\IG}{\mbox{IG}}
\newcommand{\Dir}{\mbox{Dir}}
\newcommand{\Ber}{\mbox{Ber}}
\newcommand{\Be}{\mbox{Be}}
\newcommand{\Unif}{\mbox{Unif}}
\newcommand{\Nor}{\mbox{N}}
\newcommand{\Binom}{\mbox{Bin}}
\newcommand{\Multi}{\mbox{Multinomial}}
\newcommand{\Pa}{\mbox{Pa}}
\newcommand{\NB}{\mbox{NB}}


\newcommand{\Gprime}{G^\prime}
\newcommand{\Normal}{\mathrm{N}}
\newcommand{\Poi}{\mathrm{Poi}}



\renewcommand{\th}{\theta}
\newcommand{\thstar}{\theta^\star}
\newcommand{\sig}{\sigma}
\newcommand{\sigsq}{\sig^2}

\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\indep}{\stackrel{indep}{\sim}}



\begin{document}
% ===========================================================================

%\classheader
\section*{Winter 15 -- AMS206B Homework 5}

\textbf{Due at the {\em beginning of class}, Tuesday March. 3rd }.

\noindent
%\textbf{This hw includes more questions on the topics covered in the previous two hws.  These questions will give you more chance to practice the concepts in different settings.}


\bnum

\item %#1
\begin{enumerate}
\item %#1-a
\begin{align*} f(y_2, ...,y_n|y_1,\rho , \sigma^2) = & f(y_n|y_2, ...,y_{n-1}, y_1,\rho , \sigma^2) f(y_{n-1}|y_2, ...,y_{n-2}, y_1,\rho , \sigma^2) ... f(y_2|y_1,\rho , \sigma^2) \\
= & f(y_n|y_{n-1}, \rho , \sigma^2) f(y_{n-1}|y_{n-2}, \rho , \sigma^2) ... f(y_2|y_1,\rho , \sigma^2) \\
= & N(\rho y_{n-1}, \sigma^2)N(\rho y_{n-2}, \sigma^2)...N(\rho y_{1}, \sigma^2)\\
= & {(\frac{1}{\sqrt{2\pi\sigma^2}})}^{n-1}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=2}^{n}(\rho y_{i}-y_{i-1})^2\right\}
\end{align*}
\item %#1-b
\begin{enumerate}
\item \begin{align*} f(\rho, \sigma^2 | y_1,...,y_n) \propto & f(y_1,...,y_n |\rho, \sigma^2) f(\rho, \sigma^2) \\
\propto & f(y_2, ...,y_n|y_1,\rho , \sigma^2) f(y_1|\rho , \sigma^2)f(\rho, \sigma^2) \\
\propto &  {(\frac{1}{\sqrt{2\pi\sigma^2}})}^{n-1}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=2}^{n}(\rho y_{i}-y_{i-1})^2\right\} {(\frac{1}{\sqrt{2\pi\sigma^2}})}\exp\left\{-\frac{y_1^2}{2\sigma^2}\right\}\frac{1}{\sigma^2} \\
\propto & (\frac{1}{\sqrt{\sigma^2}})^{n+2}\exp\left\{-\frac{1}{2\sigma^2}(\sum_{i=2}^{n}(\rho y_{i}-y_{i-1})^2 + y_1^2) \right\}
\end{align*}
\item \begin{align*}
p(\rho|\sigma_2, y_1,...y_n) \propto & \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=2}^{n}(\rho y_{i}-y_{i-1})^2 \right\}\\
\propto & \exp\left\{-\frac{1}{2\sigma^2} (\rho^2 \sum_{i=2}^{n} y_i^2- 2\rho\sum_{i=2}^{n} y_{i} y_{i-1}) \right\}\\
= & \exp\left\{-\frac{\sum_{i=2}^{n} y_i^2}{2\sigma^2} (\rho^2 - 2\rho\frac{\sum_{i=2}^{n} y_{i} y_{i-1}}{\sum_{i=2}^{n} y_i^2}) \right\}\\
\propto & \exp\left\{-\frac{1}{2\frac{\sigma^2}{\sum_{i=2}^{n} y_i^2}} (\rho  - \frac{\sum_{i=2}^{n} y_{i} y_{i-1}}{\sum_{i=2}^{n} y_i^2})^2 \right\}\\
\sim & N(\frac{\sum_{i=2}^{n} y_{i} y_{i-1}}{\sum_{i=2}^{n} y_i^2}, \sqrt{\frac{\sigma^2}{\sum_{i=2}^{n} y_i^2}})\\
p(\sigma^2| y_1,...y_n) \propto & \int (\frac{1}{\sqrt{\sigma^2}})^{n+2}\exp\left\{-\frac{1}{2\sigma^2}(\sum_{i=2}^{n}(\rho y_{i}-y_{i-1})^2 + y_1^2) \right\} d\rho \\
=& (\frac{1}{\sqrt{\sigma^2}})^{n+2}\exp\left\{-\frac{y_1^2}{2\sigma^2}\right\} \int \exp\left\{-\frac{1}{2\frac{\sigma^2}{\sum_{i=2}^{n} y_i^2}} (\rho  - \frac{\sum_{i=2}^{n} y_{i} y_{i-1}}{\sum_{i=2}^{n} y_i^2})^2 \right\}\\
& \times \exp\left\{\frac{1}{2\frac{\sigma^2}{\sum_{i=2}^{n} y_i^2}}(\frac{\sum_{i=2}^{n} y_{i} y_{i-1}}{\sum_{i=2}^{n} y_i^2})^2 - \frac{1}{2\frac{\sigma^2}{\sum_{i=2}^{n} y_i^2}}\frac{\sum_{i=2}^{n} y_{i-1}^2}{\sum_{i=2}^{n} y_i^2} \right\} d\rho \\
=& (\frac{1}{\sqrt{\sigma^2}})^{n+2}\exp\left\{\frac{\frac{(\sum_{i=2}^{n} y_{i} y_{i-1})^2}{\sum_{i=2}^{n} y_i^2} - \sum_{i=2}^{n} y_{i-1}^2}{2 \sigma^2}\right\} \sqrt{2\pi\frac{\sigma^2}{\sum_{i=2}^{n} y_i^2}}\\
\propto & (\sigma^2)^{-(n+1)/2}\exp\left\{\frac{\frac{(\sum_{i=2}^{n} y_{i} y_{i-1})^2}{\sum_{i=2}^{n} y_i^2} - \sum_{i=2}^{n} y_{i-1}^2}{2 \sigma^2}\right\} \\
\sim & IG(\alpha = n/2 - 1/2, \beta = \frac{-\frac{(\sum_{i=2}^{n} y_{i} y_{i-1})^2}{\sum_{i=2}^{n} y_i^2} + \sum_{i=2}^{n} y_{i-1}^2}{2}) 
\end{align*}
\end{enumerate}
\end{enumerate}

\item %#2
\bnum
\item %#2-a 
\begin{align*}
m(\textbf{x}) =& \int p(\theta_i) p(x_i|\theta_i) d\theta_i \\
=& \int \prod_{i=1}^n \frac{\beta^\alpha}{\Gamma(\alpha)} \theta_i^{\alpha-1} e^{-\beta\theta_i} e^{-\theta_i} \frac{\theta_i^{x_i}}{x_i!} d\theta_i \\
=& \prod_{i=1}^n \int \frac{\beta^\alpha}{\Gamma(\alpha)} \frac{1}{x_i!} \theta_i^{\alpha+x_i-1} e^{-(\beta+1)\theta_i} d\theta_i \\
=& \prod_{i=1}^n \frac{\beta^\alpha}{\Gamma(\alpha)} \frac{1}{x_i!} \frac{\Gamma(\alpha+x_i)}{(\beta+1)^{\alpha+x_i}} \\
=& \prod_{i=1}^n {{\alpha-1}\choose{\alpha+x_i-1}}(\frac{\beta}{\beta+1})^\alpha (\frac{1}{\beta+1})^{x_i}
\end{align*}
\item %#2-b
 Using what we know about negative binomial ,
\begin{align*}
E(x) =& \bar{x} = \frac{\alpha}{\beta} \\
Var(x) =& s^2 =  \frac{\alpha}{\beta^2}(\beta+1)\\
s^2 =& \frac{\bar{x} \beta}{\beta^2}(\beta+1) \\
\beta s^2 =& \bar{x} (\beta +1) \\
\hat{\beta} =& \frac{\bar{x}}{s^2-\bar{x}}\\
\intertext{substitute $\hat{\beta}$ back to the expectation,}
\hat{\alpha} =& \frac{s^2 - \bar{x}}{\bar{x}^2} \\ 
\end{align*}
My answer of $\alpha$ and $\beta$ are inverse of the textbook's estimate since I am using the gamma function of other reference textbook other than the Berger's one.
\enum

\item %#3
\bnum
\item %#3-a
We begin by finding the joint posterior density of $\theta$ and $\sigma^2$.
  \begin{align*}
    \operatorname{\pi}(\theta, \sigma^2 | \mathbf{x}) 
      \propto &
        \operatorname{\pi}(\theta, \sigma^2)
        \operatorname{f}(\mathbf{x}|\theta, \sigma^2) \\ 
      = & 
        \operatorname{\pi_1}(\theta | \sigma^2)
        \operatorname{\pi_2}(\sigma^2)
        \operatorname{f}(\mathbf{x}|\theta, \sigma^2) \\
      = & 
        \left[ \frac{1}{\sqrt{2\pi\tau\sigma^2}} \exp\left\{ \frac{-(\theta - \mu)^2}{2\tau\sigma^2} \right\} \right]
          \left[
            \frac{1}{\Gamma(\alpha) \beta^\alpha (\sigma^2)^{\alpha+1}} 
            \exp\left\{ \frac{-1}{\sigma^2\beta} \right\}
            \mathbb{I}(\sigma^2>0)
          \right] \\
      & \times
        \left[
          \left( \frac{1}{\sqrt{2\pi\sigma^2} } \right)^n
          \exp\left\{ \frac{-\sum_{i=1}^n (x_i - \theta)^2 }{2\sigma^2} \right\}
        \right] \\
      \intertext{Collect the $\theta$ terms:}        
      = & 
        \frac{1}{\sqrt{2\pi\tau\sigma^2}}
          \exp\left\{
            \frac{-(1 + n\tau)}{2\tau\sigma^2}
            \left( \theta^2 - 2\theta(\mu + \tau n \bar{x})(1 + n\tau)^{-1} \right)
          \right\} \\
      & \times
        \frac{1}{\Gamma(\alpha) \beta^\alpha (\sigma^2)^{\alpha+1}}
        \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n
        \exp \left\{
          \frac{-1}{\sigma^2} \left( \frac{\mu^2}{2\tau} + \frac{1}{\beta} + \frac{\sum_{i=1}^{n} x_i^2}{2} \right)
        \right\}
        \mathbb{I}(\sigma^2>0) \\
      \intertext{And complete the square for the $\theta$ terms in the exponential:}
      = &
        \frac{1}{\sqrt{2\pi\tau\sigma^2}}
          \exp\left\{
            \frac{-(1 + n\tau)}{2\tau\sigma^2}
            \left[
              \left(\theta - (\mu+\tau n \bar{x})(1+n\tau)^{-1}\right)^2 - \left((\mu+\tau n \bar{x})(1+n \tau)^{-1}\right)^2
            \right]
          \right\} \\
      & \times
        \frac{1}{\Gamma(\alpha) \beta^\alpha (\sigma^2)^{\alpha+1}}
        \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n
        \exp \left\{
          \frac{-1}{\sigma^2} \left( \frac{\mu^2}{2\tau} + \frac{1}{\beta} + \frac{\sum_{i=1}^{n} x_i^2}{2} \right)
        \right\}
        \mathbb{I}(\sigma^2>0) \\
      = &
        \frac{1}{\sqrt{2\pi\tau\sigma^2}}
          \exp\left\{
            \frac{-\left(\theta - (\mu+\tau n \bar{x})(1+n\tau)^{-1}\right)^2}{2\sigma^2(\tau^{-1}+n)^{-1}}
          \right\} \\
      & \times
        \frac{1}{\Gamma(\alpha) \beta^\alpha (\sigma^2)^{\alpha+1}}
        \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n \\
      & \times
        \underbrace{
          \exp \left\{
            \frac{-1}{\sigma^2}
            \left[
              \left(\frac{\mu^2}{2\tau} + \frac{1}{\beta} + \frac{\sum_{i=1}^{n} x_i^2}{2}\right)
              - \frac{(\mu + \tau n \bar{x})^2}{2\tau(1 + n \tau)}
            \right]
          \right\}
        }_{A}
        \mathbb{I}(\sigma^2>0) \\
      \intertext{Consider the quantity $A$ separately:}
    A
      = &
        \exp \left\{
          \frac{-1}{\sigma^2}
          \left[
            \frac{\mu^2}{2\tau} + \frac{1}{\beta} + \frac{1}{2}\sum_{i=1}^{n} x_i^2
            - \frac{1}{2}n\bar{x}^2 + \frac{1}{2}n\bar{x}^2
            - \frac{(\mu + \tau n \bar{x})^2}{2\tau(1 + n \tau)}
          \right]
        \right\} \\
      = &
        \exp \left\{
          \frac{-1}{\sigma^2}
          \left[
            \frac{\mu^2}{2\tau} + \frac{1}{\beta}
            + \frac{1}{2}\sum_{i=1}^n\left( x_i - \bar{x} \right)^2
            + \frac{1}{2}n\bar{x}^2 - \frac{(\mu + \tau n \bar{x})^2}{2\tau(1 + n \tau)}
          \right]
        \right\} \\
      \intertext{where in the last step we used the identity $\sum_{i=1}^n(x_i-\bar{x})^2 = \sum_{i=1}^n x_i^2 - n \bar{x}^2$.  After some simplification:}
      = &
        \exp \left\{
          \frac{-1}{\sigma^2}
          \left[
            \frac{\mu^2}{2\tau} + \frac{1}{\beta}
            + \frac{1}{2}\sum_{i=1}^n\left( x_i - \bar{x} \right)^2
            + \frac{n\tau(\mu^2 - 2\mu\bar{x} + \bar{x}^2)}{2\tau(1+n\tau)}
          \right]
        \right\} \\
      = &
        \exp \left\{
          \frac{-1}{\sigma^2}
          \left[
            \frac{\mu^2}{2\tau} + \frac{1}{\beta}
            + \frac{1}{2}\sum_{i=1}^n\left( x_i - \bar{x} \right)^2
            + \frac{n(\mu-\bar{x})^2}{2(1+n\tau)}
          \right]
        \right\} \\
      \intertext{Plugging back in for $A$ we have:}
    \operatorname{\pi}(\theta, \sigma^2 | \mathbf{x}) 
      \propto &
        \frac{1}{\sqrt{2\pi\tau\sigma^2}}
          \exp\left\{
            \frac{-\left(\theta - (\mu+\tau n \bar{x})(1+n\tau)^{-1}\right)^2}{2\sigma^2(\tau^{-1}+n)^{-1}}
          \right\} \\
      & \times
        \frac{1}{\Gamma(\alpha) \beta^\alpha (\sigma^2)^{\alpha+1}}
        \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n \\
      & \times
        \exp \left\{
          \frac{-1}{\sigma^2}
          \left[
            \frac{\mu^2}{2\tau} + \frac{1}{\beta}
            + \frac{1}{2}\sum_{i=1}^n\left( x_i - \bar{x} \right)^2
            + \frac{n(\mu-\bar{x})^2}{2(1+n\tau)}
          \right]
        \right\}
        \mathbb{I}(\sigma^2>0) \\
      = &  
        \frac{1}{\sqrt{2\pi\tau\sigma^2}}
          \exp\left\{
            \frac{-\left(\theta - (\mu+\tau n \bar{x})(1+n\tau)^{-1}\right)^2}{2\sigma^2(\tau^{-1}+n)^{-1}}
          \right\} \\
      & \times
        \frac{1}{\Gamma(\alpha) \beta^\alpha (\sigma^2)^{\alpha+1}}
        \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n
        \exp \left\{
          \frac{-1}{\sigma^2 \beta'}
        \right\}
        \mathbb{I}(\sigma^2>0) \\
      = &  
        \frac{1}{\sqrt{2\pi\tau\sigma^2}}
          \exp\left\{
            \frac{-\left(\theta - (\mu+\tau n \bar{x})(1+n\tau)^{-1}\right)^2}{2\sigma^2(\tau^{-1}+n)^{-1}}
          \right\} \\
      & \times
        \left( \frac{1}{\sqrt{2\pi}} \right)^n
        \frac{1}{\Gamma(\alpha) \beta^\alpha (\sigma^2)^{\alpha+\frac{n}{2}+1}}
        \exp \left\{
          \frac{-1}{\sigma^2 \beta'}
        \right\}
        \mathbb{I}(\sigma^2>0) \\
  \end{align*}
The desired kernel for the joint distribution can easily be seen in the last equation.
\\ \\
Now consider the distribution of $\theta | \mathbf{X}, \sigma^2$.  Given that
\begin{align*}
  \theta | \sigma^2 & \sim N(\mu, \, \tau\sigma^2) \\
  X | \theta, \sigma^2 & \sim N(\theta, \, \sigma^2)
\end{align*}
for a single observation, we know:
\[ \theta | X, \sigma^2 \sim 
  N\left(
    \frac{\tau\sigma^2}{\tau\sigma^2 + \sigma^2}x + \frac{\sigma^2}{\tau\sigma^2 + \sigma^2}\mu, \,
    \left( \frac{1}{\tau\sigma^2} + \frac{1}{\sigma^2}\right)^{-1}
  \right) 
\]
We can use sufficiency and the fact that $\bar{X}|\theta \sim N(\theta, \, \frac{\sigma^2}{n})$ to find $\theta|\mathbf{X}, \sigma^2$:
\[ \theta | \bar{X}, \sigma^2 \sim 
  N\left(
    \frac{\tau\sigma^2}{\tau\sigma^2 + \sigma^2/n}\bar{x} + \frac{\sigma^2/n}{\tau\sigma^2 + \sigma^2/n}\mu, \,
    \left( \frac{1}{\tau\sigma^2} + \frac{1}{\sigma^2/n}\right)^{-1}
  \right) 
\]
And after a bit of simplification:
\[ \theta | \bar{X}, \sigma^2 \sim 
  N\left(
    (n\tau\bar{x} + \mu)(n\tau + 1)^{-1}, \,
    \sigma^2(\tau^{-1} + n)
  \right) 
\]
which is the desired density for $\operatorname{\pi}_1(\theta | \sigma^2, \, \mathbf{x})$.  The density of $\operatorname{\pi}_2(\sigma^2|\mathbf{x})$ is shown in part (b).
\item %#3-b
  Let $u(\mathbf{x}) = (\mu + n\tau\bar x)/(n\tau+1)$, $\sigma_1^2 = (\tau^{-1}+n)^{-1} \sigma^2$, and $\alpha' = \alpha+n/2$.  Then:
  \begin{IEEEeqnarray*}{rCl}
    \operatorname{\pi}(\theta, \sigma^2 | \mathbf{x}) & = & \operatorname{\pi_2}(\sigma^2 | \mathbf{x} )
                                                                                           \operatorname{\pi_1}(\theta | \mathbf{x}, \sigma^2 ) \\
                                                                                 & = & \frac{1}{\operatorname{ \Gamma}(\alpha') \beta'^{\alpha'} (\sigma^2)^{\alpha'+1} }
                                                                                           \exp \left\{ \frac{-1}{\sigma^2 \beta'} \right\}
                                                                                           \mathbb{I}(\sigma^2>0)
                                                                                           \frac{1}{ \sqrt{ 2 \pi \sigma_1^2} }
                                                                                           \exp\left\{ \frac{-(\theta - \mu(\mathbf{x}))^2}{2\sigma_1^2} \right\} \\
    \operatorname{\pi}(\sigma^2 | \mathbf{x}) & = & \frac{1}{\operatorname{ \Gamma}(\alpha') \beta'^{\alpha'} (\sigma^2)^{\alpha'+1} }
                                                                                           \exp \left\{ \frac{-1}{\sigma^2 \beta'} \right\}
                                                                                           \mathbb{I}(\sigma^2>0)
                                                                                           \underbrace{
                                                                                             \int_{-\infty}^\infty 
                                                                                               \frac{1}{ \sqrt{ 2 \pi \sigma_1^2} }
                                                                                               \exp\left\{ \frac{-(\theta - \mu(\mathbf{x}))^2}{2\sigma_1^2} \right\}
                                                                                             \, d\theta
                                                                                           }_{ \text{ =1; } \operatorname{N}(\mu(\mathbf{x}), \, \sigma_1^2) } \\
                                                                       & = & \frac{1}{\operatorname{ \Gamma}(\alpha') \beta'^{\alpha'} (\sigma^2)^{\alpha'+1} }
                                                                                           \exp \left\{ \frac{-1}{\sigma^2 \beta'} \right\}
                                                                                           \mathbb{I}(\sigma^2>0) \\
                                                                       & = & \frac{1}{\operatorname{ \Gamma}(\alpha + \frac{n}{2}) \beta'^{\alpha + \frac{n}{2}} (\sigma^2)^{\alpha + \frac{n}{2}+1} }
                                                                                           \exp \left\{ \frac{-1}{\sigma^2 \beta'} \right\}
                                                                                           \mathbb{I}(\sigma^2>0)
  \end{IEEEeqnarray*}
  From the pdf we can see that $\sigma^2 | \mathbf{X} \sim \operatorname{IG}(\alpha + n/2, \beta')$
\item %#3-c
Again let $\mu(\mathbf{x}) = (\mu + n\tau\bar x)/(n\tau+1)$ and $\alpha' = \alpha+n/2$.  Also let $\omega = (\tau^{-1} +n)^{-1}$
We are given:
\[
  \operatorname{\pi}(\theta, \sigma^2|\mathbf{x})
    = 
      \frac{1}{\sqrt{2\pi\omega\sigma^2}}
      \exp \left\{
        \frac{-(\theta-\mu(\mathbf{x}))^2}{2\omega\sigma^2}
      \right\}
      \frac{1}{\Gamma(\alpha') \beta'^{\alpha'} (\sigma^2)^{\alpha'+1}}
      \exp \left\{
        \frac{-1}{\sigma^2 \beta'}
      \right\}
      \mathbb{I}(\sigma^2>0)
\]
We integrate out $\sigma^2$ to get the marginal distribution for $\theta$ given $\mathbf{x}$:
\begin{align*}
  \operatorname{\pi}(\theta|\mathbf{x})
    = &
      \int_0^\infty
      \frac{1}{\sqrt{2\pi\omega\sigma^2}}
      \exp \left\{
        \frac{-(\theta-\mu(\mathbf{x}))^2}{2\omega\sigma^2}
      \right\}
      \frac{1}{\Gamma(\alpha') \beta'^{\alpha'} (\sigma^2)^{\alpha'+1}}
      \exp \left\{
        \frac{-1}{\sigma^2 \beta'}
      \right\}
      \, d\sigma^2 \\
    = &
      \frac{
        \Gamma(\alpha'+\frac{1}{2})
        \left[
          \left(
            \frac{(\theta-\mu(\mathbf{x}))^2}{2\omega} + \frac{1}{\beta'}
          \right)^{-1}
        \right]^{\alpha'+\frac{1}{2}}}
      {
        \sqrt{2\pi\omega} \Gamma(\alpha') \beta'^{\alpha'}
      } \\
    & \times
      \underbrace{
        \int_0^\infty
        \frac{1}{
          \Gamma(\alpha'+\frac{1}{2})
          \left[
            \left(
              \frac{(\theta-\mu(\mathbf{x}))^2}{2\omega} + \frac{1}{\beta'}
            \right)^{-1}
          \right]^{\alpha'+\frac{1}{2}}
        }
        \frac{1}{(\sigma^2)^{\alpha' + \frac{1}{2} + 1}}
        \exp \left\{
          \frac{-1}{\sigma^2}
          \left(
            \frac{\left( \theta-\mu(\mathbf{x}) \right)^2}{2\omega}
          \right)
        \right\} \, d\sigma^2
      }_{\text{=1; } \operatorname{IG}\left(\alpha' + 1/2, \, (2\omega)/((\theta-\mu(\mathbf{x}))^2)\right)} \\
    = &
      \frac{
        \Gamma(\alpha'+\frac{1}{2})
        \left[
          \frac{(\theta-\mu(\mathbf{x}))^2}{2\omega {\beta'}^{-1}} + 1
        \right]^{-\alpha'-\frac{1}{2}}
        \left(
          \frac{1}{\beta'}
        \right)^{-\alpha'-\frac{1}{2}}
      }
      {
        \sqrt{2\pi\omega} \Gamma(\alpha') {\beta'}^{\alpha'}
      } \\      
    = &
      \frac{
        \Gamma(\alpha'+\frac{1}{2})
        \left[
          \frac{(\theta-\mu(\mathbf{x}))^2}{2\omega {\beta'}^{-1}} + 1
        \right]^{-\alpha'-\frac{1}{2}}
      }
      {
        \sqrt{2\pi\omega} \Gamma(\alpha') {\beta'}^{-\frac{1}{2}}
      } \\   
    = &
      \frac{
        \Gamma(\alpha'+\frac{1}{2})
        \left[
          \frac{(\theta-\mu(\mathbf{x}))^2}{2\omega {\beta'}^{-1}\alpha' {\alpha'}^{-1} } + 1
        \right]^{-\alpha'-\frac{1}{2}}
      }
      {
        \sqrt{2 \pi \omega {\beta'}^{-1} \alpha' {\alpha'}^{-1}} \Gamma(\alpha')
      } \\
    \intertext{Plugging in for $\alpha'$ and $\omega$:}
    = &
      \frac{
        \Gamma\left(\alpha+\frac{n}{2}+\frac{1}{2}\right)
        \left[
          \frac{(\theta-\mu(\mathbf{x}))^2}{2 (\tau^{-1} +n)^{-1} {\beta'}^{-1}(\alpha+\frac{n}{2}){(\alpha+\frac{n}{2})}^{-1} } + 1
        \right]^{-\left(\alpha+\frac{n}{2}\right)-\frac{1}{2}}
      }
      {
        \sqrt{2 \pi (\tau^{-1} +n)^{-1} {\beta'}^{-1} (\alpha+\frac{n}{2}){(\alpha+\frac{n}{2})}^{-1}}
        \Gamma\left(\alpha+\frac{n}{2}\right)
      } \\
    = &
      \frac{
        \Gamma\left(
          \frac{2\alpha + n + 1}{2}
        \right)
        \left[
          \frac{
            (\theta-\mu(\mathbf{x}))^2
          }
          {
            (2\alpha + n)\left[ (\tau^{-1}+n)(\alpha+\frac{n}{2})\beta' \right]^{-1}
          } + 1
          \right]^{-\left( \frac{2\alpha + n + 1}{2} \right)}
      }
      {
        \left[
          (2\alpha+n)\pi \left[ (\tau^{-1} + n)(\alpha+\frac{n}{2}) \beta' \right]^{-1}
        \right]^{\frac{1}{2}}
        \Gamma\left(
          \frac{2\alpha + n}{2}
        \right)
      }
\end{align*}
which is the density of $t( 2\alpha + n, \, \mu(\mathbf{x}), \, [(\tau^{-1} + n)(\alpha + n/2)\beta']^{-1})$ as desired.
\item %#3-d
The joint prior density in this problem is from a conjugate family for the distribution of $\mathbf{X}$ because $\pi_1(\theta|\sigma^2)$ is in the same family as $\pi_1(\theta|\sigma^2, \mathbf{x})$, and $\pi_2(\sigma^2)$ is in the same family as $\pi_2(\sigma^2 | \mathbf{x})$.
\enum

\newpage

\item %#4
\bnum 
\item %#4-a
To calculate the posterior mean and variance we must first find the posterior. \\
\begin{align*}
\pi(\theta|X) &=\frac{ f(X| \theta) \pi (\theta)}{m(X)} \\
\pi(\theta|X) & \propto f(X| \theta) \pi (\theta) \\
& \propto {n \choose x} \theta^{x} (1-\theta)^{n-x} \left( \frac{\Gamma (\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \right) \theta^{\alpha-1}(1-\theta)^{\beta-1} \\
& \propto \theta^{x} (1-\theta)^{n-x} \theta^{\alpha-1}(1-\theta)^{\beta-1}  \\
& \propto \theta^{\alpha+x-1}(1-\theta)^{\beta+n-x-1}
\end{align*}
Which is the kernal of a $\beta(\alpha+x,\beta+n-x-1)$ distribution.  Therefore, the $E[\theta|X]$ and $Var(\theta|X)$ are known.


\begin{align*}
E[\theta|X] &= \frac{x+ \alpha}{x+\alpha+\beta+n-x} \nonumber \\
E[\theta|X] &= \frac{x+ \alpha}{\alpha+\beta+n} \tag{4.1} \\
Var(\theta|X) &= \frac{(x+ \alpha)(\beta+n-x)}{(\alpha+\beta+n)^2(\beta+n-x+1)} \tag{4.2}
\end{align*}






\item %#4-b
To calculate the posterior mean and variance we must first find the posterior. \\
\begin{align*}
\pi(\theta|X) &=\frac{ f(X| \theta) \pi (\theta)}{m(X)} \\
\pi(\theta|X) & \propto f(X| \theta) \pi (\theta) \\
& \propto \left(\frac{1}{\theta} \right)^n exp\left(-\frac{\sum \limits_{i=1}^n x_i}{\theta} \right) \frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{-\alpha-1} exp\left(-\frac{\beta}{\theta}\right) \\
& \propto \left(\frac{1}{\theta} \right)^n exp\left(-\frac{\sum \limits_{i=1}^n x_i}{\theta} \right) \theta^{-\alpha-1} exp\left(-\frac{\beta}{\theta}\right)  \\
& \propto  \theta^{-\alpha-n-1} exp\left[-\frac{1}{\theta}\left(\sum \limits_{i=1}^n x_i+\beta \right) \right]  
\end{align*}

Which is the kernal of a $IG\left(\alpha+n,\sum \limits_{i=1}^n x_i+\beta \right)$ distribution.  Therefore, the $E[\theta|X]$ and $Var(\theta|X)$ are known.


\begin{align*}
E[\theta|X] &= \frac{\sum \limits_{i=1}^n x_i+\beta}{\alpha+n-1}  \tag{4.3} \\
Var(\theta|X) &= \frac{\sum \limits_{i=1}^n x_i+\beta}{(\alpha+n-1)^2(\alpha+n-2)}  \tag{4.4}
\end{align*}

\item %#4-c

To calculate the posterior mean and variance we must first find the posterior. \\
\begin{align*}
\pi(\theta|X) &=\frac{ f(X| \theta) \pi (\theta)}{m(X)} \\
\pi(\theta|X) & \propto f(X| \theta) \pi (\theta) \\
& \propto \frac{x^{n/2-1}}{(2\theta)^{n/2} \Gamma(n/2)}exp\left\{-\frac{x}{2\theta} \right\}  \frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{-\alpha-1} exp\left(-\frac{\beta}{\theta}\right) \\
& \propto \left(\frac{1}{(2)^{n/2}} \right) \left(\frac{1}{(\theta)^{n/2}} \right) exp\left\{-\frac{x}{2\theta} \right\} \theta^{-\alpha-1} exp\left(-\frac{\beta}{\theta}\right)  \\
& \propto  \theta^{-\alpha-n/2-1} exp\left\{-\frac{1}{\theta}\left(x/2+\beta \right) \right\}  
\end{align*}

Which is the kernal of a $IG\left(\alpha+n/2,x/2+\beta \right)$ distribution.  Therefore, the $E[\theta|X]$ and $Var(\theta|X)$ are known.


\begin{align*}
E[\theta|X] &= \frac{x/2+\beta}{\alpha+n/2-1}  \tag{4.5} \\
Var(\theta|X) &= \frac{x/2+\beta}{(\alpha+n/2-1)^2(\alpha+n/2-2)}  \tag{4.6}
\end{align*}

\item %#4-d
Part A \\
To find the MAP, we need to maximize the posterior ($\pi(\theta|x)$). Here the maximum by was found by taking the derivative with respect to $\theta$ of $\ln (\pi(\theta|x))$.  
\begin{align*}
\pi(\theta|X) &=\frac{ f(X| \theta) \pi (\theta)}{m(X)} \\
\pi(\theta|X) &=\left(\frac{1}{m(X)} \right)  {n \choose x} \theta^{x} (1-\theta)^{n-x} \left( \frac{\Gamma (\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \right) \theta^{\alpha-1}(1-\theta)^{\beta-1} \\
\ln (\pi(\theta|X)) &= \ln \left(\frac{1}{m(X)}{n \choose x} \right)+\ln \left( \frac{\Gamma (\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \right) +(x+\alpha-1) \ln \theta +(\beta+n-x-1) \ln(1-\theta) \\
\frac{\partial}{\partial \theta} \left( \ln [\pi(\theta|X)] \right) &= 0+0+\frac{x+\alpha-1}{\theta}-\frac{\beta+n-x-1}{1-\theta} \\
\frac{\partial}{\partial \theta} \left( \ln [\pi(\theta|X)] \right) &=\frac{x+\alpha-1}{\theta}-\frac{\beta+n-x-1}{1-\theta}
\end{align*}
Then , for this case $\frac{\partial}{\partial \theta} \left( \ln [\pi(\theta|X)] \right)$=0 is indeed a maximum.  
\begin{align*}
0 &=\frac{x+\alpha-1}{\hat{\theta}_{MAP}}-\frac{\beta+n-x-1}{1-\hat{\theta}_{MAP}} \\
0 &=(x+\alpha-1)(1-\hat{\theta}_{MAP})-(\beta+n-x-1)(\hat{\theta}_{MAP}) \\
(\beta+n-x-1)(\hat{\theta}_{MAP})+(x+\alpha-1)(\hat{\theta}_{MAP}) &= x+\alpha-1 \\
(\alpha+\beta+n-2)(\hat{\theta}_{MAP}) &= x+\alpha-1 \\
\hat{\theta}_{MAP} &= \frac{x+\alpha-1}{\alpha+\beta+n-2} 
\end{align*}
It was given that $Var(\hat{\theta}_{MAP})=E[(\hat{\theta}_{MAP}-\theta))^2|X]=E[(\hat{\theta}_{MAP}-E(\theta|X))^2|X]+Var(\theta|X)$ and $E(\theta|X)$ and $Var(\theta|X)$ were found in 4.1 and 4.2. 
\begin{align*}
Var(\hat{\theta}_{MAP}) &= E\left[(\hat{\theta}_{MAP}-E(\theta|X))^2|X\right]+Var\left(\theta|X\right) \\
Var(\hat{\theta}_{MAP}) &= E\left[\left(\frac{x+\alpha-1}{\alpha+\beta+n-2} -\frac{x+ \alpha}{\alpha+\beta+n}\right)^2|X\right]+\frac{(x+ \alpha)(\beta+n-x)}{(\alpha+\beta+n)^2(\beta+n-x+1)} \\
Var(\hat{\theta}_{MAP}) &= \left(\frac{x+\alpha-1}{\alpha+\beta+n-2} -\frac{x+ \alpha}{\alpha+\beta+n}\right)^2+\frac{(x+ \alpha)(\beta+n-x)}{(\alpha+\beta+n)^2(\beta+n-x+1)}
\end{align*}

\vspace{5mm}
Part B \\
To find the MAP, we need to maximize the posterior ($\pi(\theta|x)$). Here the maximum by was found by taking the derivative with respect to $\theta$ of $\ln (\pi(\theta|x))$.  
\begin{align*}
\pi(\theta|X) &=\frac{ f(X| \theta) \pi (\theta)}{m(X)} \\
\pi(\theta|X) &=\left(\frac{1}{m(X)} \right) \left(\frac{1}{\theta} \right)^n exp\left(-\frac{\sum \limits_{i=1}^n x_i}{\theta} \right) \frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{-\alpha-1} exp\left(-\frac{\beta}{\theta}\right) \\
\ln (\pi(\theta|X)) &= \ln \left(\frac{\beta^\alpha}{m(X) \Gamma(\alpha)} \right) +(-\alpha-n-1) \ln \theta -\frac{\sum_{i=1}^{n} x_i+\beta}{\theta} \\
\frac{\partial}{\partial \theta} \left( \ln [\pi(\theta|X)] \right) &= 0+\frac{-\alpha-n-1}{\theta}+\frac{\sum_{i=1}^{n} x_i+\beta}{\theta^2} \\
\frac{\partial}{\partial \theta} \left( \ln [\pi(\theta|X)] \right) &=-\frac{\alpha+n+1}{\theta}+\frac{\sum_{i=1}^{n} x_i+\beta}{\theta^2}
\end{align*}
Then , for this case $\frac{\partial}{\partial \theta} \left( \ln [\pi(\theta|X)] \right)$=0 is indeed a maximum.  
\begin{align*}
0 &=-\frac{\alpha+n+1}{\hat{\theta}_{MAP}}+\frac{\sum_{i=1}^{n} x_i+\beta}{\hat{\theta}_{MAP}^2} \\
(\alpha+n+1)(\hat{\theta}_{MAP}) &=\sum_{i=1}^{n} x_i+\beta \\
\hat{\theta}_{MAP} &= \frac{\sum_{i=1}^{n} x_i+\beta}{\alpha+n+1} 
\end{align*}
It was given that $Var(\hat{\theta}_{MAP})=E[(\hat{\theta}_{MAP}-\theta))^2|X]=E[(\hat{\theta}_{MAP}-E(\theta|X))^2|X]+Var(\theta|X)$ and $E(\theta|X)$ and $Var(\theta|X)$ were found in 4.3 and 4.4. 
\begin{align*}
Var(\hat{\theta}_{MAP}) &= E\left[(\hat{\theta}_{MAP}-E(\theta|X))^2|X\right]+Var\left(\theta|X\right) \\
Var(\hat{\theta}_{MAP}) &= E \left[ \left( \frac{\sum_{i=1}^{n} x_i+\beta}{\alpha+n+1} -\frac{\sum_{i=1}^n x_i+\beta}{\alpha+n-1} \right)^2 |X \right]+\frac{\sum_{i=1}^n x_i+\beta}{(\alpha+n-1)^2(\alpha+n-2)} \\
Var(\hat{\theta}_{MAP}) &=  \left( \frac{\sum_{i=1}^{n} x_i+\beta}{\alpha+n+1} -\frac{\sum_{i=1}^n x_i+\beta}{\alpha+n-1} \right)^2 +\frac{\sum_{i=1}^n x_i+\beta}{(\alpha+n-1)^2(\alpha+n-2)}
\end{align*}

\vspace{5mm}
Part C \\
To find the MAP, we need to maximize the posterior ($\pi(\theta|x)$). Here the maximum by was found by taking the derivative with respect to $\theta$ of $\ln (\pi(\theta|x))$.  
\begin{align*}
\pi(\theta|X) &=\frac{ f(X| \theta) \pi (\theta)}{m(X)} \\
\pi(\theta|X) &=\left(\frac{1}{m(X)} \right) \frac{x^{n/2-1}}{(2\theta)^{n/2} \Gamma(n/2)}exp\left\{-\frac{x}{2\theta} \right\}  \frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{-\alpha-1} exp\left(-\frac{\beta}{\theta}\right) \\
\ln (\pi(\theta|X)) &= \ln \left(\frac{\beta^\alpha x^{n/2-1} }{2^{n/2} m(X) \Gamma(\alpha) \Gamma(n/2)}  \right) -\frac{x}{2 \theta}-(\alpha+n/2+1) \ln \theta -\frac{\beta}{\theta} \\
\frac{\partial}{\partial \theta} \left( \ln [\pi(\theta|X)] \right) &= 0+\frac{x+2\beta}{2\theta^2} -\frac{\alpha+n/2+1}{\theta}  \\
\frac{\partial}{\partial \theta} \left( \ln [\pi(\theta|X)] \right) &= \frac{x+2\beta}{2\theta^2} -\frac{\alpha+n/2+1}{\theta}
\end{align*}
Then , for this case $\frac{\partial}{\partial \theta} \left( \ln [\pi(\theta|X)] \right)$=0 is indeed a maximum.  
\begin{align*}
0 &=\frac{x+2\beta}{2\hat{\theta}_{MAP}^2} -\frac{\alpha+n/2+1}{\hat{\theta}_{MAP}}\\
\frac{\alpha+n/2+1}{\hat{\theta}_{MAP}}&=\frac{x+2\beta}{2\hat{\theta}_{MAP}^2}\\
(2\alpha+m+2)(\hat{\theta}_{MAP}) &= x+2\beta \\
\hat{\theta}_{MAP}&=\frac{x+2\beta}{2\alpha+m+2}
\end{align*}
It was given that $Var(\hat{\theta}_{MAP})=E[(\hat{\theta}_{MAP}-\theta))^2|X]=E[(\hat{\theta}_{MAP}-E(\theta|X))^2|X]+Var(\theta|X)$ and $E(\theta|X)$ and $Var(\theta|X)$ were found in 4.5 and 4.6. 
\begin{align*}
Var(\hat{\theta}_{MAP}) &= E\left[(\hat{\theta}_{MAP}-E(\theta|X))^2|X\right]+Var\left(\theta|X\right) \\
Var(\hat{\theta}_{MAP}) &= E \left[ \left( \frac{x+2\beta}{2\alpha+m+2} -\frac{x/2+\beta}{\alpha+n/2-1} \right)^2 |X \right]+\frac{x/2+\beta}{(\alpha+n/2-1)^2(\alpha+n/2-2)}  \\
Var(\hat{\theta}_{MAP}) &=  \left( \frac{x+2\beta}{2\alpha+m+2} -\frac{x/2+\beta}{\alpha+n/2-1} \right)^2 +\frac{x/2+\beta}{(\alpha+n/2-1)^2(\alpha+n/2-2)} 
\end{align*}


\enum

\pagebreak

\item %#5 
Part I \\

Since $Y$ is the $n^{th}$ order statistic, and there is a general formula for order statistics then the formula can be applied.  However, since the formula requires the CDF, the CDF must be found first.
\begin{align*}
f(x|\theta)&=\frac{1}{\theta}I(0<x<\theta)  \\
F(x|\theta)&=\int_{0}^{x} \frac{1}{\theta}d\theta \\
F(x|\theta)&= \frac{x}{\theta}
\end{align*}
So $F(x|\theta)=\begin{cases}
0 & x \leq 0 \\
\frac{x}{\theta} & 0<x < \theta \\
1 & x> \theta 
\end{cases}$

Then $f_Y(y)={n-1 \choose n-1}\left( \frac{n}{\theta} \right)\left(\frac{1}{\theta}y \right)^{n-1}I(0<x<1)$ or $f(y)=\left( \frac{n}{\theta} \right)\left(\frac{y}{\theta} \right)^{n-1}I(0<x<1)$. \\

Part II \\

It is known that the posterior mean minimizes least square error or is the ``Bayes' point estimate'' for least square error .  Therefore, it is necessary to compute the posterior and its expectation.  

The posterior is $\pi(\theta|Y) =\frac{ f(Y| \theta) \pi (\theta)}{m(Y)}$ and $m(Y)=\int f(Y| \theta) \pi (\theta) d \theta.$ Since $0<y<\theta$, then $\theta>y$.  

\begin{align*}
m(Y)&=\int_{y}^{\infty} \left( \frac{n}{\theta} \right)\left(\frac{y}{\theta} \right)^{n-1} \frac{\beta \alpha^\beta}{\theta^{\beta+1}} d \theta \\
m(Y)&= \beta \alpha^\beta n y^{n-1} \int_{y}^{\infty} \left( \frac{1}{\theta} \right)^{n+\beta+1} d \theta
\end{align*}
Since $n\geq 1$ and $\beta>0$ than $n+\beta+1 \geq 1$ so the power rule can be applied for integration.
\begin{align*}
m(Y)&= \beta \alpha^\beta n y^{n-1} \left[-\frac{1}{n+\beta} \left(\frac{1}{\theta^{n+\beta}} \right) \right]_y^{\infty} \\
m(Y)&= \beta \alpha^\beta n y^{n-1} \left( \frac{1}{y^{n+\beta}} \right) 
\end{align*}

Now, the full posterior can be computed.
\begin{align*}
\pi(\theta|Y) &=\frac{ f(Y| \theta) \pi (\theta)}{m(Y)} \\
\pi(\theta|Y) &=\frac{\left( \frac{n}{\theta} \right)\left(\frac{y}{\theta} \right)^{n-1} \frac{\beta \alpha^\beta}{\theta^{\beta+1}}}{\beta \alpha^\beta n y^{n-1} \left( \frac{1}{y^{n+\beta}} \right) } \\
\pi(\theta|Y) & (n + \beta) \left( \frac{1}{\theta} \right)^{n+\beta+1} y^{n+\beta} 
\end{align*}
From this, the posterior mean or the ``Bayes' point estimator'' for mean squared loss can be computed.
\begin{align*}
E[\theta|Y] &= \int_{y}^\infty (n+\beta) y^{n+\beta}\left(  \frac{1}{\theta} \right)^{n+\beta} d\theta 
\end{align*}
Since a data set smaller then one would not be consider and $beta>0$ than $n+\beta>1$, allowing application of the power rule again. \begin{align*}
E[\theta|Y] &=  (n+\beta) y^{n+\beta} \left[ - \left( \frac{1}{n+\beta-1} \right) \left(  \frac{1}{\theta} \right)^{n+\beta-1} \right]_y^\infty \\
E[\theta|Y] &= \frac{(n+\beta) y^{n+\beta}}{(n+\beta-1)y^{n+\beta-1}} \\
E[\theta|Y] &= \frac{(n+\beta) y}{(n+\beta-1)} 
\end{align*}
So the Bayes' point estimate for $\theta$ given mean squared error loss is $y\frac{n+\beta}{n+\beta-1}$. 

\newpage


\item %#6
  $\frac{1}{\sigma^2} \sim \operatorname{Ga}(\frac{r}{2}, \frac{2}{r})$ implies $\sigma^2 \sim \operatorname{IG}(\frac{r}{2}, \frac{2}{r})$
  \begin{IEEEeqnarray*}{rCl}
    \operatorname{m}(x) & = & \int_0^\infty \operatorname{f}(x|\sigma^2) \operatorname{\pi}(\sigma^2) \, d\sigma^2 \\
                                      & = & \int_0^\infty \frac{ 1 }{ \sqrt{2\pi\sigma^2} }  \exp\left\{ \frac{-x^2}{2\sigma^2} \right\}
                                                  \frac{1}{\operatorname{\Gamma} \left( \frac{r}{2} \right) \left( \sigma^2 \right)^{\frac{r}{2} +1 } } 
                                                  \exp\left\{ \frac{-1}{\sigma^2 \left( \frac{2}{r} \right) } \right\} \, d\sigma^2 \\
                                      & = & \frac{
                                                         \operatorname{\Gamma} \left( \frac{r}{2} + \frac{1}{2} \right)
                                                         \left( \frac{x^2}{2} + \frac{r}{2} \right)^{-\frac{r}{2} - \frac{1}{2}}
                                                       }
                                                       {
                                                         \operatorname{\Gamma}\left( \frac{r}{2} \right) 
                                                         \left( \frac{2}{r} \right)^{ \frac{r}{2} } \left( 2 \pi \right)^{\frac{1}{2}}
                                                       }
                                                       \underbrace{
                                                          \int_0^\infty
                                                           \frac{
                                                                    1
                                                                  }
                                                                  {
                                                                    \operatorname{\Gamma} \left( \frac{r}{2} + \frac{1}{2} \right)
                                                                    \left( \frac{x^2}{2} + \frac{r}{2} \right)^{-\frac{r}{2} - \frac{1}{2}}
                                                                  }
                                                           \frac{1}{ (\sigma^2)^{\frac{r}{2} + \frac{1}{2} + 1 } }
                                                           \exp\left\{ \frac{-1}{\sigma^2} \left( \frac{x^2}{2} + 
                                                           \frac{r}{2} \right) \right\}
                                                         \, d\sigma^2
                                                         }_{\text{=1; } \operatorname{IG}\left(\frac{r}{2} + \frac{1}{2}, \left(\frac{x^2}{2} + \frac{r}{2}\right)^{-1} \right)} \\
                                       & = & \frac{
                                                          \operatorname{\Gamma}\left( \frac{r+1}{2} \right) \left( \frac{\frac{x^2}{r} + 1}{\frac{2}{r}} \right)^{-\frac{r}{2} - \frac{1}{2}}
                                                         }
                                                         {
                                                           \operatorname{\Gamma}\left( \frac{r}{2} \right) \left( \frac{2}{r} \right)^{\frac{r}{2}} \left( 2\pi \right)^{\frac{1}{2}}
                                                         } \\
                                       & = & \frac{
                                                          \operatorname{\Gamma}\left( \frac{r+1}{2} \right) \left( \frac{x^2}{r} + 1\right)^{-\frac{r}{2} - \frac{1}{2}} 
                                                          \left( \frac{2}{r} \right)^{\frac{r}{2} + \frac{1}{2}} 
                                                        }
                                                        {
                                                          \operatorname{\Gamma}\left( \frac{r}{2} \right) \left( \frac{2}{r} \right)^{\frac{r}{2}} \left( 2\pi \right)^{\frac{1}{2}}
                                                        } \\
                                       & = & \frac{
                                                          \operatorname{\Gamma}\left( \frac{r+1}{2} \right) \left( \frac{x^2}{r} + 1\right)^{-\left( \frac{r + 1}{2} \right)} 
                                                        }
                                                        {
                                                          \operatorname{\Gamma}\left( \frac{r}{2} \right) \left( \frac{2}{r} \right)^{-\frac{1}{2}} \left( 2\pi \right)^{\frac{1}{2}}
                                                        } \\
                                       & = & \frac{
                                                          \operatorname{\Gamma}\left( \frac{r+1}{2} \right) \left( \frac{x^2}{r} + 1\right)^{-\left( \frac{r + 1}{2} \right)} 
                                                        }
                                                        {
                                                          \operatorname{\Gamma}\left( \frac{r}{2} \right) \left( r\pi \right)^{\frac{1}{2}}
                                                        }
  \end{IEEEeqnarray*}
  This is the pdf for a $t$-distribution with $r$ degrees of freedom
\item %#7

\enum



% ===========================================================================

\end{document}
